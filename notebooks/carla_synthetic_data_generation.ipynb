{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f0e532d-0adf-48df-97e7-0947ad809f55",
   "metadata": {},
   "source": [
    "# Synthetic Data Generation for Smart Cities using CARLA and Cosmos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c30209f-885c-420c-9f38-cf3b09023796",
   "metadata": {},
   "source": [
    "## Overview \n",
    "\n",
    "This guide demonstrates how to leverage the opensource CARLA simulator to simulate various kinds of traffic patterns and incidents at a variety of map locations.\n",
    "\n",
    "The simulations are then augmented to generate datasets needed for training and finetuning models. \n",
    "\n",
    "\n",
    "<img src=\"../data/docs/main_workflow.png\" width=\"1156\" height=\"408\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc562f50-5ca6-4c57-96ec-fb874b96d3e3",
   "metadata": {},
   "source": [
    "This notebook assumes all the necessary NIMs and Carla server are spun up in order to consume as part of the docker compose example. Here we will now do some simple checks to see if the required services are available to proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202d3a08-000f-4675-baa0-b4fc6d51c08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -fsS http://$NIM_HOST:$VLM_PORT/v1/health/ready | grep -q \"Service is ready.\" && echo \"VLM is healthy\" || echo \"VLM is unhealthy\"\n",
    "! curl -fsS http://$NIM_HOST:$LLM_PORT/v1/health/ready | grep -q \"Service is ready.\" && echo \"LLM is healthy\" || echo \"LLM is unhealthy\"\n",
    "! curl -fsS http://$NIM_HOST:$TRANSFER_GRADIO_PORT/ | grep -q 'window.gradio_config' && echo \"Transfer gradio is healthy\" || echo \"Transfer gradio is unhealthy\"\n",
    "! nc -zv $CARLA_HOST $CARLA_PORT | echo \"Carla server is healthy\" || echo \"Carla server is unhealthy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d647c3-4bcd-4167-bee9-23c095f4f663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages for the overall flow \n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import glob \n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import sys\n",
    "sys.path.append('/workspace/modules/carla-ground-truth-generation')\n",
    "from som import process_video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f59c93-f9d7-4616-af46-7bdecea0ff81",
   "metadata": {},
   "source": [
    "## Stage 1 - Generate Ground Truth \n",
    "\n",
    "This workflow uses the open source [Carla](https://carla.org/) simulator to simulate various kinds of traffic patterns and incidents at a variety of map locations. The current SDG release is based on Carla 0.9.16. This stage takes in 3 pieces of information: An unreal engine map to run the simulation in, a scenario log containing the actor playback information (car/pedestrian movements), and a sensor config that defines where the cameras are placed and what info they should record. Samples of all 3 of these files can be found in this repo, please see the README for more information on creating your own. \n",
    "\n",
    "<img src=\"../data/docs/Stage1.png\" width=\"438\" height=\"324\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a37deb1-e64f-4b53-86c5-5e534dbc2221",
   "metadata": {},
   "source": [
    "The notebook comes with a default global config that helps control the parameters needed for ground truth generation. Ensure to adjust it based on the requirements for SDG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76a3b74-8c26-49fc-8aa4-0318c59f8fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /tmp/wf-config.json\n",
    "{\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 2000,\n",
    "    \"timeout\": 360.0,\n",
    "    \"time_factor\": 1.0,\n",
    "    \"generate_videos\": true,\n",
    "    \"limit_distance\": 100.0,\n",
    "    \"area_threshold\": 100,\n",
    "    \"class_filter_config\": \"config/filter_semantic_classes.yaml\",\n",
    "    \"ignore_hero\": false,\n",
    "    \"move_spectator\": false,\n",
    "    \"detect_collisions\": true,\n",
    "    \"output_dir\": \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d96304-c199-4299-91fd-c834046244ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup some environment variables that will be needed later\n",
    "%env SCENARIO_DIR=/workspace/data/examples\n",
    "%env CARLA_OUTPUT_DIR=/workspace/data/outputs/CARLA\n",
    "%env COSMOS_OUTPUT_DIR=/workspace/data/outputs/Cosmos\n",
    "%env POSTPROCESS_OUTPUT_DIR=/workspace/data/outputs/\n",
    "%env RUN_ID=default_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1627b28-0652-4960-bf8a-0b2a5a32478b",
   "metadata": {},
   "source": [
    "Now that all of the proper prerequisites are set, we can run the simulation and generate ground truth data. This will run simulation of all log files in SCENARIO_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a497c6f9-beea-49cf-baad-2754347d532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!/workspace/modules/carla-ground-truth-generation/batch_processing.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdc25fc",
   "metadata": {},
   "source": [
    "Once our generation is done we can view a snapshot of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f486ff41-d211-4ef2-8608-8f23c7980dd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the base directory to your ground truth folder\n",
    "GROUND_TRUTH_DIR = os.path.join(os.getenv('CARLA_OUTPUT_DIR'), os.getenv('RUN_ID'), 'scenario_1')\n",
    "\n",
    "def visualize_image_grid(base_dir):\n",
    "    subfolders = sorted(d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d)) and d != \"odvg\")[:9]\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "    \n",
    "    for ax, folder in zip(axes.flatten(), subfolders):\n",
    "        folder_path = os.path.join(base_dir, folder)\n",
    "        images = sorted(f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')))\n",
    "        if images:\n",
    "            ax.imshow(Image.open(os.path.join(folder_path, images[-1])))\n",
    "            ax.set_title(folder)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'No images found', ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    for ax in axes.flatten()[len(subfolders):]:\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_image_grid(GROUND_TRUTH_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0203eae2-0187-4fa7-8716-f65750d37602",
   "metadata": {},
   "source": [
    "## Stage 2 : Augmentation with Cosmos Transfer2.5\n",
    "\n",
    "In stage 2, we'll take the ground truth data generated by carla and augment it to expand our dataset variety. Below, you will define a list of variables and a number of augmentations to create. For each augmentation, 1 random condition will be chosen per variable. These conditions will be applied to the original scenario creating a brand new video that maintains the core information from the ground truth simulation.\n",
    "\n",
    "<img src=\"../data/docs/Stage2.png\" width=\"542\" height=\"326\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b31344-b540-4de5-80c0-920a5546f185",
   "metadata": {},
   "source": [
    "Let us now set up the configuration necessary to perform augmentation. These will determine what gets generated by Cosmos Transfer. If using the default files no changes are needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfd4fe2-e82a-4363-a4d5-c22baa0d1c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER-EDITABLE VARIABLES\n",
    "# ==============================================================================\n",
    "BASE_INPUT_DIR = os.path.join(os.getenv('CARLA_OUTPUT_DIR'), os.getenv('RUN_ID'))\n",
    "BASE_OUTPUT_DIR = os.path.join(os.getenv('COSMOS_OUTPUT_DIR'), os.getenv('RUN_ID'))\n",
    "CONFIG_FILE_PATH = \"/workspace/modules/augmentation/configs/config_carla.yaml\"\n",
    "NUM_AUGMENTATIONS = 2\n",
    "\n",
    "# Endpoint and model configuration\n",
    "VLM_URL = f\"http://{os.getenv('NIM_HOST')}:{os.getenv('VLM_PORT')}/v1\" \n",
    "LLM_URL = f\"http://{os.getenv('NIM_HOST')}:{os.getenv('LLM_PORT')}/v1\"\n",
    "COSMOS_URL = f\"http://{os.getenv('NIM_HOST')}:{os.getenv('TRANSFER_GRADIO_PORT')}/\"\n",
    "\n",
    "#Cosmos variables\n",
    "variables = {\n",
    "            'weather_condition': ['clear_sky', 'overcast', 'snow_falling', 'raining', 'foggy'],\n",
    "            'lighting_condition': ['sunrise', 'sunset', 'twilight', 'mid_morning', 'afternoon', 'zenith', 'golden_hour', 'blue_hour', 'night'],\n",
    "            'road_condition': ['dry', 'snow', 'sand', 'puddles', 'flooding']\n",
    "            }\n",
    "\n",
    "with open(CONFIG_FILE_PATH) as f:\n",
    "    default_config = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "print(f'Default configuration: ', default_config)\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d637462-7cf2-4944-b140-5559c7983cae",
   "metadata": {},
   "source": [
    "Finally we'll input our variables and write a new config. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ff8ffb-101b-492f-a732-e780428a9616",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "scenario_dirs = sorted([\n",
    "    d for d in os.listdir(BASE_INPUT_DIR)\n",
    "    if os.path.isdir(os.path.join(BASE_INPUT_DIR, d)) and d.startswith('scenario_')\n",
    "])\n",
    "for scenario_name in scenario_dirs:\n",
    "    video_dir = os.path.join(BASE_INPUT_DIR, scenario_name, \"videos\")\n",
    "    \n",
    "    if os.path.exists(os.path.join(video_dir, \"rgb.mp4\")):\n",
    "        for i in range(NUM_AUGMENTATIONS):\n",
    "            entry = {\n",
    "                \"inputs\": {\n",
    "                    \"rgb\": os.path.join(video_dir, \"rgb.mp4\"),\n",
    "                    \"controls\": {\n",
    "                        \"edge\": os.path.join(video_dir, \"edges.mp4\"),\n",
    "                        \"depth\": os.path.join(video_dir, \"depth.mp4\"),\n",
    "                        \"seg\": os.path.join(video_dir, \"semantic_segmentation.mp4\"),\n",
    "                    }\n",
    "                },\n",
    "                \"output\": {\n",
    "                    \"video\": os.path.join(BASE_OUTPUT_DIR, scenario_name, f\"Augmentation_{i}\", f\"output.mp4\"),\n",
    "                    \"caption\": os.path.join(BASE_OUTPUT_DIR, scenario_name, f\"Augmentation_{i}\", f\"output.txt\"),\n",
    "                    \"metadata\": os.path.join(BASE_OUTPUT_DIR, scenario_name, f\"Augmentation_{i}\", f\"output.json\"),\n",
    "                }\n",
    "            }\n",
    "            data_list.append(entry)\n",
    "\n",
    "config = default_config\n",
    "config['data']=data_list \n",
    "config['endpoints'] = {\n",
    "    'vlm': {'url': VLM_URL, 'model': 'nvidia/cosmos-reason1-7b'},\n",
    "    'llm': {'url': LLM_URL, 'model': 'nvidia/nvidia-nemotron-nano-9b-v2'},\n",
    "    'cosmos': {'url': COSMOS_URL, 'model': 'Cosmos-Transfer2.5-2B'}\n",
    "}\n",
    "\n",
    "# Optionally you can switch to use public larger NIMs for the VLM/LLM using build nvidia to have better prompts for your data generation\n",
    "# config['endpoints']['llm']['url'] = 'https://integrate.api.nvidia.com/v1'\n",
    "# config['endpoints']['llm']['model'] = 'nvidia/llama-3.3-nemotron-super-49b-v1'\n",
    "\n",
    "config['template_generation']['system_prompt_file']='/workspace/modules/augmentation/configs/prompts/carla_template_generation_system_prompt.txt'\n",
    "config['cosmos']['configuration']='/workspace/modules/augmentation/configs/cosmos_configs/config_template.toml'\n",
    "config['cosmos']['parameters']['inference_name']='nvidia/Cosmos-Transfer2.5-2B'\n",
    "\n",
    "CONFIG_FILE_PATH='/workspace/data/outputs/augmentation_config.yaml'\n",
    "with open(CONFIG_FILE_PATH, 'w') as f:\n",
    "    yaml.dump(config, f, sort_keys=False, indent=2)\n",
    "\n",
    "print(f\"Successfully wrote config to {CONFIG_FILE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb434db0-70d6-4b29-b7aa-35e509a12ff3",
   "metadata": {},
   "source": [
    "### Run the Augmentation.\n",
    "\n",
    "Run the augmentation script using the config defined above. Depending on your hardware configuration this step may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4007d05c-7086-4902-993d-b670009a2b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "uv run /workspace/modules/augmentation/modules/cli.py --config /workspace/data/outputs/augmentation_config.yaml "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68db373",
   "metadata": {},
   "source": [
    "### View The Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d86b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "#Replace the Path with the augmentation and scenario you'd like to view\n",
    "Video(\"../data/outputs/Cosmos/default_run/scenario_1/Augmentation_0/output.mp4\", width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badc4c2f-da6f-464e-8ed2-813725d3df39",
   "metadata": {},
   "source": [
    "## Stage 3 : Post-Processing for Finetuning\n",
    "\n",
    "At this point, we have successfully created a ground truth dataset, and augmented it to increase variety. The final step is to package all this information up for actual use in model training of finetuning. To do this we'll perform 2 actions: generate SOM overlays and Q&A pairs. \n",
    "\n",
    "SOM (set of marks) is a structured labeling approach where points of interest are annotated with discrete marks or identifiers. In our case we will add bounding boxes as well as numeric IDs to specific cars involved in the incident. These additional labels help ground the VLM, improving the quality of fine-tuning.\n",
    "\n",
    "Q&A pairs are text prompts and responses automatically generated from the ground-truth data. They provide a useful mechanism for fine-tuning VLMs by enabling the model to learn from the dataset in a semi-supervised or self-supervised manner.\n",
    "\n",
    "<img src=\"../data/docs/Stage3.png\" width=\"469\" height=\"234\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5615c3ed",
   "metadata": {},
   "source": [
    "### Generate SOM Videos\n",
    "When generating ground truth data with Carla, collision events are detected and saved. Using this data, we can create specific overlays for vehicles involved in incidents. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d396f45e-c389-40d8-85e7-b85f818b8aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Prepare SOM overlays for CARLA ground truth \n",
    "for scenario in os.listdir(BASE_INPUT_DIR):\n",
    "    rgb_video_path = os.path.join(BASE_INPUT_DIR, scenario, 'videos/rgb.mp4')\n",
    "    for event in glob.glob(os.path.join(BASE_INPUT_DIR, scenario, \"events*.json\")):\n",
    "        event_name = os.path.basename(event).split('.')[0]\n",
    "        target_som_path = os.path.join(BASE_INPUT_DIR, scenario, f'{event_name}_rgb_som.mp4')\n",
    "        odvg_path = os.path.join(BASE_INPUT_DIR, scenario, \"odvg\")\n",
    "        if not os.path.exists(target_som_path):\n",
    "            process_video(rgb_video_path, odvg_path, target_som_path)\n",
    "\n",
    "# Prepare SOM overlays for Cosmos augmented videos \n",
    "for video in glob.glob(os.path.join(BASE_OUTPUT_DIR, 'scenario*', 'Augmentation*', 'output.mp4')):\n",
    "    metadata_path = video.replace('.mp4', '.json')\n",
    "    if os.path.exists(metadata_path):\n",
    "        with open(metadata_path) as f:\n",
    "            metadata = json.loads(f.read())\n",
    "            gt_video_path = metadata['original_video_path']\n",
    "            gt_path = os.path.dirname(os.path.dirname(gt_video_path))\n",
    "            odvg_path = os.path.join(gt_path, 'odvg')\n",
    "            for event in glob.glob(os.path.join(gt_path, \"events*.json\")):\n",
    "                som_video_name = os.path.basename(event.replace('.json', '.mp4')).split('.')\n",
    "                som_video_name[0] += '_som'\n",
    "                som_video_name = '.'.join(som_video_name)\n",
    "                target_som_path = os.path.join(os.path.dirname(video), som_video_name)\n",
    "                event_name = os.path.basename(event).split('.')[0]\n",
    "                if not os.path.exists(target_som_path):\n",
    "                    process_video(video, odvg_path, target_som_path)\n",
    "            # All bbox som \n",
    "            target_som_path = os.path.join(os.path.dirname(video), 'output_allcars_som.mp4')\n",
    "            if not os.path.exists(target_som_path):\n",
    "                process_video(video, odvg_path, target_som_path, area_threshold=5000)\n",
    "                \n",
    "print('Completed generating SOM overlays for Carla and Cosmos videos!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43dc1d1",
   "metadata": {},
   "source": [
    "### View SOM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4c00b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "video_path = \"../data/outputs/Cosmos/default_run/scenario_1/Augmentation_0/output_allcars_som.mp4\"\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, total_frames - 1)\n",
    "ret, frame = cap.read()\n",
    "cap.release()\n",
    "\n",
    "frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "img = Image.fromarray(frame_rgb)\n",
    "display(img.resize((800, int(img.height * 800 / img.width)), Image.LANCZOS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8d4713-1557-4841-ae33-f1356af235a1",
   "metadata": {},
   "source": [
    "### Generate Q&A pairs\n",
    "Using our overlayed videos, we can generate a Q&A dataset for finetuning a VLM. Since we know which vehicles are invlolved in incidents we can create a large number of simple yes or no questions grounded in our videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3e3e5f-7b84-41e6-b09c-1fec899e023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "uv run /workspace/modules/postprocess/postprocess_for_vlm.py --carla_folder /workspace/data/outputs/CARLA \\\n",
    "                                                             --cosmos_folder /workspace/data/outputs/Cosmos \\\n",
    "                                                             --output_folder /workspace/data/outputs/postprocess \\\n",
    "                                                             --run_id $RUN_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cc6a52-332b-491c-b568-0fbd3035e6c1",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "This concludes the content for this repo. For more information on finetuning Cosmos Reason see [here](https://nvidia-cosmos.github.io/cosmos-cookbook/recipes/post_training/reason1/intelligent-transportation/post_training.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
